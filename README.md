# PySpark-ETL-Project
Building an ETL Pipeline using PySpark
Iâ€™ll develop an ETL Pipeline using PySpark to process this dataset to handle the following tasks:

Extract: Load the dataset from the CSV file.
Transform: Clean the data, handle missing values, and pivot year-wise temperature data for analysis.
Load: Save the processed data into a new storage format (e.g., Parquet or a database).

